\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{microtype}

\title{Hybrid Spiking Language Model: Combining Spike Counts and Membrane Potentials for Energy-Efficient and Noise-Robust Character Prediction}

\author{Hiroto Funasaki\\
Independent Researcher, Japan\\
\texttt{cell-activation@ymail.ne.jp}\\
GitHub: \texttt{hafufu-stack}
}

\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
I propose a novel character-level language model using Spiking Neural Networks (SNNs) that combines both spike counts and membrane potentials for output prediction. Unlike conventional SNN approaches that only use spike counts, this hybrid method leverages the analog information contained in membrane potentials, inspired by biological decision-making processes. Experimental results on character prediction tasks demonstrate that: (1) the proposed SNN achieves \textbf{42 times higher energy efficiency} than equivalent DNNs; (2) SNNs show \textbf{remarkable noise robustness}, with no accuracy degradation even at 30\% input noise; and (3) the hybrid spike + membrane potential approach provides more information for prediction. These findings establish SNNs as a promising architecture for energy-efficient and robust language processing on neuromorphic hardware.
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) have revolutionized natural language processing but require massive computational resources. Spiking Neural Networks (SNNs), inspired by biological neurons, offer a fundamentally different computing paradigm based on discrete spike events rather than continuous activations.

Previous work has demonstrated SNNs for various tasks, but language modeling with SNNs remains largely unexplored. A key limitation of conventional SNN approaches is that they only use spike counts (firing rates) for output computation, discarding the rich analog information contained in membrane potentials.

I propose a \textbf{Hybrid Spiking Language Model} that uses both spike counts and membrane potentials for next-character prediction. This approach is inspired by my previous finding that combining digital (spikes) and analog (membrane potential) information significantly improves decision-making accuracy in brain-inspired models.

\section{Related Work}

\subsection{Spiking Neural Networks}
SNNs use Leaky Integrate-and-Fire (LIF) neurons that integrate input currents and emit discrete spikes when the membrane potential crosses a threshold. This event-driven computation offers potential energy savings on neuromorphic hardware.

\subsection{Neural Language Models}
Traditional language models use DNNs, RNNs, or Transformers with continuous activations. While effective, these require dense matrix operations that consume significant power.

\section{Proposed Method}

\subsection{SNN Reservoir Architecture}
The model uses a reservoir computing approach with LIF neurons:
\begin{equation}
    \tau \frac{dv_i}{dt} = -(v_i - v_{rest}) + \sum_j W_{ij} s_j + W^{in}_i x_t
\end{equation}
where $v_i$ is the membrane potential, $s_j$ are incoming spikes, and $x_t$ is the input character embedding.

\subsection{Traditional Readout (Spike-only)}
Conventional approaches compute output as:
\begin{equation}
    y_t = W^{out} \cdot \mathbf{s}_t
\end{equation}
where $\mathbf{s}_t$ is the spike count vector.

\subsection{Hybrid Readout (Spike + Membrane)}
I propose combining spike counts and normalized membrane potentials:
\begin{equation}
    y_t = W^{spike} \cdot \mathbf{s}_t + W^{membrane} \cdot \tilde{\mathbf{v}}_t
\end{equation}
where $\tilde{\mathbf{v}}_t$ is the normalized membrane potential vector.

\subsection{Intuition}
Spike counts are discrete (0, 1, 2, ...) while membrane potentials are continuous. A neuron with spike count 5 and membrane potential near threshold conveys different information than one with the same spike count but resting potential.

\section{Experiments}

\subsection{Setup}
\begin{itemize}
    \item \textbf{Task}: Character-level next-character prediction
    \item \textbf{Data}: English text, 22,920 characters
    \item \textbf{Models}: SNN (LIF neurons), DNN (tanh), LSTM
    \item \textbf{Sizes}: 300, 500, 600 neurons/hidden units
\end{itemize}

\subsection{Accuracy Comparison}
\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Model & Accuracy & Perplexity & Operations \\
\midrule
SNN-300 & 10.24\% & 28.20 & 93.85M \\
DNN-300 & 15.22\% & 46.27 & 5,885M \\
LSTM-300 & 18.77\% & 41.28 & 23,542M \\
\bottomrule
\end{tabular}
\caption{Accuracy and computational cost comparison}
\end{table}

\subsection{Energy Efficiency}
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Model & Efficiency (Acc/MOps) & Relative \\
\midrule
\textbf{SNN-300} & \textbf{0.1091} & \textbf{1.0×} \\
DNN-300 & 0.0026 & 42× worse \\
LSTM-300 & 0.0008 & 136× worse \\
\bottomrule
\end{tabular}
\caption{Energy efficiency comparison}
\end{table}

\textbf{Finding 1: SNN is 42× more energy-efficient than DNN.}

\subsection{Noise Robustness}
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Model & 0\% Noise & 30\% Noise & Degradation \\
\midrule
\textbf{SNN-500} & 8.67\% & 8.80\% & \textbf{-0.13\%} \\
DNN-500 & 10.83\% & 7.77\% & +3.06\% \\
LSTM-300 & 18.77\% & 17.71\% & +1.05\% \\
\bottomrule
\end{tabular}
\caption{Noise robustness (input corruption)}
\end{table}

\textbf{Finding 2: SNN shows no degradation at 30\% noise—even slight improvement!}

\subsection{Hybrid vs Traditional}
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Approach & Accuracy & Perplexity \\
\midrule
Traditional (Spike only) & 11.51\% & 105.76 \\
\textbf{Hybrid (Spike + Membrane)} & \textbf{12.14\%} & \textbf{11.83} \\
\bottomrule
\end{tabular}
\caption{Hybrid vs Traditional readout}
\end{table}

\textbf{Finding 3: Hybrid approach improves perplexity by 9×!}

\section{Discussion}

\subsection{Why SNN is Energy Efficient}
\begin{enumerate}
    \item \textbf{Sparse computation}: Only spiking neurons contribute
    \item \textbf{Event-driven}: No continuous matrix operations
    \item \textbf{Addition-based}: Fewer multiplications than DNNs
\end{enumerate}

\subsection{Why SNN is Noise Robust}
\begin{enumerate}
    \item \textbf{Threshold mechanism}: Small noise doesn't trigger spikes
    \item \textbf{Membrane potential smoothing}: Absorbs transient noise
    \item \textbf{Chaotic flexibility}: Network adapts to noise
\end{enumerate}

\subsection{Why Hybrid Works}
The membrane potential carries ``readiness to fire'' information that spike counts lose. Two neurons with the same spike count may have very different membrane states, and this information helps prediction.

\subsection{Practical Implications}
\begin{itemize}
    \item \textbf{Neuromorphic chips}: SNN can run efficiently on Intel Loihi, IBM TrueNorth
    \item \textbf{Edge AI}: Noise robustness suits noisy sensor environments
    \item \textbf{Low-power NLP}: 42× efficiency enables battery-powered language processing
\end{itemize}

\section{Conclusion}

I have demonstrated that:
\begin{enumerate}
    \item \textbf{SNN is 42× more energy-efficient} than equivalent DNNs for language modeling
    \item \textbf{SNN is remarkably noise-robust}, showing no degradation at 30\% input noise
    \item \textbf{Hybrid spike + membrane potential readout} improves prediction quality
\end{enumerate}

These findings establish SNNs as a promising direction for energy-efficient and robust natural language processing, particularly for edge devices and neuromorphic hardware.

\textbf{Disclaimer}: This is a proof-of-concept study. Further benchmarking on standard datasets and hardware measurements are needed.

\section*{Data Availability}
Source code: \url{https://github.com/hafufu-stack/snn-language-model}

\begin{thebibliography}{9}
\bibitem{maass2002} Maass, W. et al. (2002). Real-time computing without stable states. \textit{Neural Computation}.
\bibitem{loihi2018} Davies, M. et al. (2018). Loihi: A Neuromorphic Manycore Processor. \textit{IEEE Micro}.
\bibitem{snnlm2024} Zhu, R. et al. (2024). SpikeBERT: Spiking Neural Networks for Language Understanding. \textit{arXiv}.
\end{thebibliography}

\end{document}
